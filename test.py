import streamlit as st
import requests, httpx, re
from bs4 import BeautifulSoup
from datetime import datetime, date, time as dtime
from zoneinfo import ZoneInfo
from concurrent.futures import ThreadPoolExecutor, as_completed

# === í‚¤ì›Œë“œ ê·¸ë£¹ ì •ì˜ ===
keyword_groups = {
    'ì‹œê²½': ['ì„œìš¸ê²½ì°°ì²­'],
    'ë³¸ì²­': ['ê²½ì°°ì²­'],
    'ì¢…í˜œë¶': [
        'ì¢…ë¡œ', 'ì¢…ì•”', 'ì„±ë¶', 'ê³ ë ¤ëŒ€', 'ì°¸ì—¬ì—°ëŒ€', 'í˜œí™”', 'ë™ëŒ€ë¬¸', 'ì¤‘ë‘',
        'ì„±ê· ê´€ëŒ€', 'í•œêµ­ì™¸ëŒ€', 'ì„œìš¸ì‹œë¦½ëŒ€', 'ê²½í¬ëŒ€', 'ê²½ì‹¤ë ¨', 'ì„œìš¸ëŒ€ë³‘ì›',
        'ë…¸ì›', 'ê°•ë¶', 'ë„ë´‰', 'ë¶ë¶€ì§€ë²•', 'ë¶ë¶€ì§€ê²€', 'ìƒê³„ë°±ë³‘ì›', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ'
    ],
    'ë§ˆí¬ì¤‘ë¶€': [
        'ë§ˆí¬', 'ì„œëŒ€ë¬¸', 'ì„œë¶€', 'ì€í‰', 'ì„œë¶€ì§€ê²€', 'ì„œë¶€ì§€ë²•', 'ì—°ì„¸ëŒ€',
        'ì‹ ì´Œì„¸ë¸Œë€ìŠ¤ë³‘ì›', 'êµ°ì¸ê¶Œì„¼í„°', 'ì¤‘ë¶€', 'ë‚¨ëŒ€ë¬¸', 'ìš©ì‚°', 'ë™êµ­ëŒ€',
        'ìˆ™ëª…ì—¬ëŒ€', 'ìˆœì²œí–¥ëŒ€ë³‘ì›'
    ],
    'ì˜ë“±í¬ê´€ì•…': [
        'ì˜ë“±í¬', 'ì–‘ì²œ', 'êµ¬ë¡œ', 'ê°•ì„œ', 'ë‚¨ë¶€ì§€ê²€', 'ë‚¨ë¶€ì§€ë²•', 'ì—¬ì˜ë„ì„±ëª¨ë³‘ì›',
        'ê³ ëŒ€êµ¬ë¡œë³‘ì›', 'ê´€ì•…', 'ê¸ˆì²œ', 'ë™ì‘', 'ë°©ë°°', 'ì„œìš¸ëŒ€', 'ì¤‘ì•™ëŒ€', 'ìˆ­ì‹¤ëŒ€', 'ë³´ë¼ë§¤ë³‘ì›'
    ],
    'ê°•ë‚¨ê´‘ì§„': [
        'ê°•ë‚¨', 'ì„œì´ˆ', 'ìˆ˜ì„œ', 'ì†¡íŒŒ', 'ê°•ë™', 'ì‚¼ì„±ì˜ë£Œì›', 'í˜„ëŒ€ì•„ì‚°ë³‘ì›',
        'ê°•ë‚¨ì„¸ë¸Œë€ìŠ¤ë³‘ì›', 'ê´‘ì§„', 'ì„±ë™', 'ë™ë¶€ì§€ê²€', 'ë™ë¶€ì§€ë²•', 'í•œì–‘ëŒ€',
        'ê±´êµ­ëŒ€', 'ì„¸ì¢…ëŒ€'
    ]
}

# === Streamlit UI ===
st.set_page_config(page_title="ë‰´ìŠ¤ í‚¤ì›Œë“œ ìˆ˜ì§‘ê¸°", layout="wide")
st.title("ğŸ“° ë‰´ìŠ¤ í‚¤ì›Œë“œ ìˆ˜ì§‘ê¸° (í†µí•©ë²„ì „)")

mode = st.radio("ê¸°ëŠ¥ ì„ íƒ", ["[ë‹¨ë…] ê¸°ì‚¬ ìˆ˜ì§‘ê¸°", "ì—°í•©ë‰´ìŠ¤Â·ë‰´ì‹œìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ê¸°"])

now = datetime.now(ZoneInfo("Asia/Seoul"))
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("ì‹œì‘ ë‚ ì§œ", value=now.date())
    start_time = st.time_input("ì‹œì‘ ì‹œê°„", value=dtime(0, 0))
with col2:
    end_date = st.date_input("ì¢…ë£Œ ë‚ ì§œ", value=now.date())
    end_time = st.time_input("ì¢…ë£Œ ì‹œê°„", value=dtime(now.hour, now.minute))

start_dt = datetime.combine(start_date, start_time).replace(tzinfo=ZoneInfo("Asia/Seoul"))
end_dt = datetime.combine(end_date, end_time).replace(tzinfo=ZoneInfo("Asia/Seoul"))

selected_groups = st.multiselect("í‚¤ì›Œë“œ ê·¸ë£¹ ì„ íƒ", list(keyword_groups.keys()), default=["ì‹œê²½", "ì¢…í˜œë¶"])
selected_keywords = [kw for group in selected_groups for kw in keyword_groups[group]]

# === ê³µí†µ í•¨ìˆ˜ ===
def highlight_keywords(text, keywords):
    for kw in keywords:
        text = re.sub(f"({re.escape(kw)})", r'<mark style="background-color: #fffb91">\1</mark>', text)
    return text

# === [ë‹¨ë…] ê¸°ì‚¬ ìˆ˜ì§‘ê¸° ===
def parse_pubdate(pubdate_str):
    try:
        return datetime.strptime(pubdate_str, "%a, %d %b %Y %H:%M:%S %z")
    except:
        return None

def extract_article_text(url):
    try:
        if "n.news.naver.com" not in url:
            return None
        html = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        soup = BeautifulSoup(html.text, "html.parser")
        content_div = soup.find("div", id="newsct_article")
        return content_div.get_text(separator="\n", strip=True) if content_div else None
    except:
        return None

def extract_media_name(url):
    try:
        domain = url.split("//")[-1].split("/")[0]
        parts = domain.split(".")
        composite_key = f"{parts[-3]}.{parts[-2]}" if len(parts) >= 3 else parts[0]
        return {
            "chosun": "ì¡°ì„ ", "joongang": "ì¤‘ì•™", "donga": "ë™ì•„", "hani": "í•œê²¨ë ˆ",
            "khan": "ê²½í–¥", "hankookilbo": "í•œêµ­", "segye": "ì„¸ê³„", "seoul": "ì„œìš¸",
            "kmib": "êµ­ë¯¼", "munhwa": "ë¬¸í™”", "kbs": "KBS", "sbs": "SBS",
            "imnews": "MBC", "jtbc": "JTBC", "ichannela": "ì±„ë„A", "tvchosun": "TVì¡°ì„ ",
            "mk": "ë§¤ê²½", "sedaily": "ì„œê²½", "hankyung": "í•œê²½", "news1": "ë‰´ìŠ¤1",
            "newsis": "ë‰´ì‹œìŠ¤", "yna": "ì—°í•©", "mt": "ë¨¸íˆ¬", "weekly": "ì£¼ê°„ì¡°ì„ ",
            "biz.chosun": "ì¡°ì„ ë¹„ì¦ˆ", "fnnews": "íŒŒë‰´"
        }.get(composite_key, composite_key.upper())
    except:
        return "[ë§¤ì²´ì¶”ì¶œì‹¤íŒ¨]"

def fetch_and_filter(item, start_dt, end_dt, selected_keywords, use_keyword_filter):
    title = BeautifulSoup(item["title"], "html.parser").get_text()
    if "[ë‹¨ë…]" not in title:
        return None
    pub_dt = parse_pubdate(item.get("pubDate"))
    if not pub_dt or not (start_dt <= pub_dt <= end_dt):
        return None
    link = item.get("link")
    if not link or "n.news.naver.com" not in link:
        return None
    body = extract_article_text(link)
    if not body:
        return None
    matched_keywords = [kw for kw in selected_keywords if kw in body] if use_keyword_filter else []
    if use_keyword_filter and not matched_keywords:
        return None
    highlighted_body = highlight_keywords(body, matched_keywords).replace("\n", "<br><br>")
    media = extract_media_name(item.get("originallink", ""))
    return {
        "ë§¤ì²´": media,
        "ì œëª©": title,
        "ë‚ ì§œ": pub_dt.strftime("%Y-%m-%d %H:%M:%S"),
        "ë³¸ë¬¸": body,
        "í•„í„°ì¼ì¹˜": ", ".join(matched_keywords),
        "ë§í¬": link,
        "í•˜ì´ë¼ì´íŠ¸": highlighted_body
    }

def run_dandok():
    use_filter = st.checkbox("ğŸ“ í‚¤ì›Œë“œ í¬í•¨ ê¸°ì‚¬ë§Œ í•„í„°ë§", value=True)
    if st.button("âœ… [ë‹¨ë…] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘"):
        client_id = "R7Q2OeVNhj8wZtNNFBwL"
        client_secret = "49E810CBKY"
        headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret
        }
        st.info("ìˆ˜ì§‘ ì¤‘...")
        all_articles = []
        for start in range(1, 1001, 100):
            res = requests.get(
                "https://openapi.naver.com/v1/search/news.json",
                headers=headers,
                params={"query": "[ë‹¨ë…]", "display": 100, "start": start, "sort": "date"},
                timeout=5
            )
            if res.status_code != 200: break
            items = res.json().get("items", [])
            with ThreadPoolExecutor(max_workers=10) as ex:
                futures = [ex.submit(fetch_and_filter, item, start_dt, end_dt, selected_keywords, use_filter) for item in items]
                for f in as_completed(futures):
                    result = f.result()
                    if result:
                        all_articles.append(result)
                        st.markdown(f"**â–³{result['ë§¤ì²´']}/{result['ì œëª©']}**")
                        st.caption(result["ë‚ ì§œ"])
                        st.markdown(f"ğŸ”— [ì›ë¬¸ ë³´ê¸°]({result['ë§í¬']})")
                        if result["í•„í„°ì¼ì¹˜"]:
                            st.write(f"**ì¼ì¹˜ í‚¤ì›Œë“œ:** {result['í•„í„°ì¼ì¹˜']}")
                        st.markdown(f"- {result['í•˜ì´ë¼ì´íŠ¸']}", unsafe_allow_html=True)
        if all_articles:
            st.subheader("ğŸ“‹ ë³µì‚¬ìš© í…ìŠ¤íŠ¸")
            text = ""
            for art in all_articles:
                clean_title = re.sub(r"\[ë‹¨ë…\]|\(ë‹¨ë…\)|ã€ë‹¨ë…ã€‘|â“§ë‹¨ë…|^ë‹¨ë…\s*[:-]?", "", art['ì œëª©']).strip()
                text += f"â–³{art['ë§¤ì²´']}/{clean_title}\n- {art['ë³¸ë¬¸']}\n\n"
            st.code(text.strip(), language="markdown")

# === ì—°í•©ë‰´ìŠ¤/ë‰´ì‹œìŠ¤ ìˆ˜ì§‘ê¸° ===
def get_article_content(url, selector):
    try:
        res = httpx.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5.0)
        soup = BeautifulSoup(res.text, "html.parser")
        el = soup.select_one(selector)
        return el.get_text(separator="\n", strip=True) if el else ""
    except:
        return ""

def fetch_news_articles(source_name, url_template, list_selector, title_selector, time_selector, time_format, content_selector, domain_prefix=""):
    articles, page = [], 1
    while True:
        url = url_template.format(page=page)
        res = httpx.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        soup = BeautifulSoup(res.text, "html.parser")
        items = soup.select(list_selector)
        if not items:
            break
        for item in items:
            title_tag = item.select_one(title_selector)
            time_tag = item.select_one(time_selector)
            if not (title_tag and time_tag): continue
            try:
                dt = datetime.strptime(time_tag.text.strip(), time_format).replace(tzinfo=ZoneInfo("Asia/Seoul"))
            except:
                continue
            if dt < start_dt:
                return articles
            if dt <= end_dt:
                href = title_tag.get("href")
                full_url = domain_prefix + href if domain_prefix else href
                articles.append({
                    "source": source_name,
                    "title": title_tag.get_text(strip=True),
                    "datetime": dt,
                    "url": full_url
                })
        page += 1
    return articles

def run_newsis_yonhap():
    if st.button("ğŸ“¥ ì—°í•©ë‰´ìŠ¤Â·ë‰´ì‹œìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘"):
        collected = []
        newsis = fetch_news_articles(
            "ë‰´ì‹œìŠ¤", "https://www.newsis.com/realnews/?cid=realnews&day=today&page={page}",
            "ul.articleList2 > li", "p.tit > a", "p.time",
            "%Y.%m.%d %H:%M:%S", "div.viewer", "https://www.newsis.com"
        )
        yonhap = fetch_news_articles(
            "ì—°í•©ë‰´ìŠ¤", "https://www.yna.co.kr/news/{page}?site=navi_latest_depth01",
            "ul.list01 > li[data-cid]", ".title01", ".txt-time",
            "%Y-%m-%d %H:%M", "div.story-news.article", "https://www.yna.co.kr"
        )
        collected = newsis + yonhap
        st.success(f"âœ… {len(collected)}ê±´ ìˆ˜ì§‘ë¨")
        if collected:
            st.subheader("ğŸ“‹ ë³µì‚¬ìš© í…ìŠ¤íŠ¸")
            text_block = ""
            for art in collected:
                content = get_article_content(art["url"], "div.viewer" if art["source"] == "ë‰´ì‹œìŠ¤" else "div.story-news.article")
                if any(kw in content for kw in selected_keywords):
                    st.markdown(f"**[{art['title']}]({art['url']})**")
                    st.caption(f"{art['datetime'].strftime('%Y-%m-%d %H:%M')} | {art['source']}")
                    st.markdown(highlight_keywords(content, selected_keywords).replace("\n", "<br>"), unsafe_allow_html=True)
                    st.markdown("---")
                    text_block += f"â–³{art['title']}\n- {content.strip()[:300]}\n\n"
            st.code(text_block.strip(), language="markdown")

# === ì‹¤í–‰ ===
if mode == "[ë‹¨ë…] ê¸°ì‚¬ ìˆ˜ì§‘ê¸°":
    run_dandok()
else:
    run_newsis_yonhap()
