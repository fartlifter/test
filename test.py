import streamlit as st
import requests, httpx, re
from bs4 import BeautifulSoup
from datetime import datetime, date, time as dtime
from zoneinfo import ZoneInfo
from concurrent.futures import ThreadPoolExecutor, as_completed

# ===== í‚¤ì›Œë“œ ê·¸ë£¹ =====
keyword_groups = {
    'ì‹œê²½': ['ì„œìš¸ê²½ì°°ì²­'],
    'ë³¸ì²­': ['ê²½ì°°ì²­'],
    'ì¢…í˜œë¶': ['ì¢…ë¡œ', 'ì¢…ì•”', 'ì„±ë¶', 'ê³ ë ¤ëŒ€', 'ì°¸ì—¬ì—°ëŒ€', 'í˜œí™”', 'ë™ëŒ€ë¬¸', 'ì¤‘ë‘',
              'ì„±ê· ê´€ëŒ€', 'í•œêµ­ì™¸ëŒ€', 'ì„œìš¸ì‹œë¦½ëŒ€', 'ê²½í¬ëŒ€', 'ê²½ì‹¤ë ¨', 'ì„œìš¸ëŒ€ë³‘ì›',
              'ë…¸ì›', 'ê°•ë¶', 'ë„ë´‰', 'ë¶ë¶€ì§€ë²•', 'ë¶ë¶€ì§€ê²€', 'ìƒê³„ë°±ë³‘ì›', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ'],
    'ë§ˆí¬ì¤‘ë¶€': ['ë§ˆí¬', 'ì„œëŒ€ë¬¸', 'ì„œë¶€', 'ì€í‰', 'ì„œë¶€ì§€ê²€', 'ì„œë¶€ì§€ë²•', 'ì—°ì„¸ëŒ€', 'ì‹ ì´Œì„¸ë¸Œë€ìŠ¤ë³‘ì›',
               'êµ°ì¸ê¶Œì„¼í„°', 'ì¤‘ë¶€', 'ë‚¨ëŒ€ë¬¸', 'ìš©ì‚°', 'ë™êµ­ëŒ€', 'ìˆ™ëª…ì—¬ëŒ€', 'ìˆœì²œí–¥ëŒ€ë³‘ì›'],
    'ì˜ë“±í¬ê´€ì•…': ['ì˜ë“±í¬', 'ì–‘ì²œ', 'êµ¬ë¡œ', 'ê°•ì„œ', 'ë‚¨ë¶€ì§€ê²€', 'ë‚¨ë¶€ì§€ë²•', 'ì—¬ì˜ë„ì„±ëª¨ë³‘ì›',
                 'ê³ ëŒ€êµ¬ë¡œë³‘ì›', 'ê´€ì•…', 'ê¸ˆì²œ', 'ë™ì‘', 'ë°©ë°°', 'ì„œìš¸ëŒ€', 'ì¤‘ì•™ëŒ€', 'ìˆ­ì‹¤ëŒ€', 'ë³´ë¼ë§¤ë³‘ì›'],
    'ê°•ë‚¨ê´‘ì§„': ['ê°•ë‚¨', 'ì„œì´ˆ', 'ìˆ˜ì„œ', 'ì†¡íŒŒ', 'ê°•ë™', 'ì‚¼ì„±ì˜ë£Œì›', 'í˜„ëŒ€ì•„ì‚°ë³‘ì›', 'ê°•ë‚¨ì„¸ë¸Œë€ìŠ¤ë³‘ì›',
               'ê´‘ì§„', 'ì„±ë™', 'ë™ë¶€ì§€ê²€', 'ë™ë¶€ì§€ë²•', 'í•œì–‘ëŒ€', 'ê±´êµ­ëŒ€', 'ì„¸ì¢…ëŒ€']
}

# ===== ë‚ ì§œ ë° í‚¤ì›Œë“œ ì„ íƒ UI =====
st.set_page_config(page_title="ë‰´ìŠ¤ í‚¤ì›Œë“œ í†µí•© ìˆ˜ì§‘ê¸°", layout="wide")
st.title("ğŸ“° ë‰´ìŠ¤ í‚¤ì›Œë“œ í†µí•© ìˆ˜ì§‘ê¸°")

now = datetime.now(ZoneInfo("Asia/Seoul"))
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("ì‹œì‘ ë‚ ì§œ", value=now.date())
    start_time = st.time_input("ì‹œì‘ ì‹œê°", value=dtime(0, 0))
with col2:
    end_date = st.date_input("ì¢…ë£Œ ë‚ ì§œ", value=now.date())
    end_time = st.time_input("ì¢…ë£Œ ì‹œê°", value=dtime(now.hour, now.minute))

start_dt = datetime.combine(start_date, start_time).replace(tzinfo=ZoneInfo("Asia/Seoul"))
end_dt = datetime.combine(end_date, end_time).replace(tzinfo=ZoneInfo("Asia/Seoul"))

selected_groups = st.multiselect("ğŸ“š í‚¤ì›Œë“œ ê·¸ë£¹", list(keyword_groups.keys()), default=['ì‹œê²½', 'ì¢…í˜œë¶'])
selected_keywords = [kw for g in selected_groups for kw in keyword_groups[g]]
use_filter = st.checkbox("ğŸ“ í‚¤ì›Œë“œ í¬í•¨ ê¸°ì‚¬ë§Œ ë³´ê¸°", value=True)

options = st.multiselect("ğŸ§­ ì‹¤í–‰í•  ìˆ˜ì§‘ê¸° ì„ íƒ", ["[ë‹¨ë…] ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°", "ì—°í•©ë‰´ìŠ¤Â·ë‰´ì‹œìŠ¤ ìˆ˜ì§‘ê¸°"])
run_btn = st.button("âœ… ì„ íƒí•œ ìˆ˜ì§‘ê¸° ì‹¤í–‰")

# ===== ì§„í–‰ ìƒíƒœ í‘œì‹œ ì˜ì—­ =====
status_area = st.empty()
progress_area = st.empty()

# ===== [ë‹¨ë…] ë‰´ìŠ¤ ìˆ˜ì§‘ =====
def fetch_dandok_news():
    status_area.info("ğŸ” [ë‹¨ë…] ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    progress = progress_area.progress(0.0)

    client_id = "R7Q2OeVNhj8wZtNNFBwL"
    client_secret = "49E810CBKY"
    headers = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}

    results = []
    seen_links = set()
    idx = 0

    for start_index in range(1, 1001, 100):
        params = {
            "query": "[ë‹¨ë…]",
            "sort": "date",
            "display": 100,
            "start": start_index
        }
        res = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
        if res.status_code != 200: break
        items = res.json().get("items", [])
        if not items: break

        for item in items:
            title = BeautifulSoup(item["title"], "html.parser").get_text()
            link = item.get("link")
            if "[ë‹¨ë…]" not in title or not link or "n.news.naver.com" not in link:
                continue
            pub_str = item.get("pubDate")
            pub_dt = datetime.strptime(pub_str, "%a, %d %b %Y %H:%M:%S %z")
            if not (start_dt <= pub_dt <= end_dt):
                continue
            html = requests.get(link, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
            soup = BeautifulSoup(html.text, "html.parser")
            body = soup.find("div", id="newsct_article")
            if not body:
                continue
            text = body.get_text("\n", strip=True)
            matched = [kw for kw in selected_keywords if kw in text] if use_filter else []
            if use_filter and not matched: continue
            results.append({
                "title": title, "link": link, "date": pub_dt.strftime("%Y-%m-%d %H:%M"),
                "content": text, "matched": matched
            })
            idx += 1
            progress.progress(min(idx / 300, 1.0), text=f"[ë‹¨ë…] {idx}ê±´ ìˆ˜ì§‘ë¨")

    status_area.success(f"âœ… [ë‹¨ë…] ê¸°ì‚¬ {idx}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    progress_area.empty()
    return results

# ===== ì—°í•©ë‰´ìŠ¤ Â· ë‰´ì‹œìŠ¤ ìˆ˜ì§‘ =====
def fetch_press_news():
    status_area.info("ğŸ” ì—°í•©ë‰´ìŠ¤Â·ë‰´ì‹œìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
    collected = []
    progress = progress_area.progress(0.0)
    count = 0

    def fetch(url, selector):
        try:
            res = httpx.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5.0)
            soup = BeautifulSoup(res.text, "html.parser")
            el = soup.select_one(selector)
            return el.get_text("\n", strip=True) if el else ""
        except:
            return ""

    def parse_articles(article_list, selector):
        nonlocal count
        results = []
        total = len(article_list)
        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = {executor.submit(fetch, a['url'], selector): a for a in article_list}
            for i, future in enumerate(as_completed(futures)):
                a = futures[future]
                try:
                    content = future.result()
                    if any(kw in content for kw in selected_keywords):
                        a['content'] = content
                        results.append(a)
                        count += 1
                        progress.progress((i + 1) / total, text=f"ë³¸ë¬¸ ìˆ˜ì§‘ {i+1}/{total}")
                except:
                    continue
        return results

    # ì—°í•©ë‰´ìŠ¤
    page, yonhap = 1, []
    while True:
        url = f"https://www.yna.co.kr/news/{page}?site=navi_latest_depth01"
        soup = BeautifulSoup(httpx.get(url).text, "html.parser")
        items = soup.select("ul.list01 > li[data-cid]")
        if not items: break
        for item in items:
            title = item.select_one(".title01")
            time_ = item.select_one(".txt-time")
            cid = item.get("data-cid")
            if not title or not time_ or not cid: continue
            dt = datetime.strptime(f"{start_dt.year}-{time_.text.strip()}", "%Y-%m-%d %H:%M").replace(tzinfo=ZoneInfo("Asia/Seoul"))
            if dt < start_dt: break
            if dt > end_dt: continue
            yonhap.append({
                "source": "ì—°í•©ë‰´ìŠ¤", "datetime": dt, "title": title.text.strip(),
                "url": f"https://www.yna.co.kr/view/{cid}"
            })
        page += 1
    collected += parse_articles(yonhap, "div.story-news.article")

    # ë‰´ì‹œìŠ¤
    page, newsis = 1, []
    while True:
        url = f"https://www.newsis.com/realnews/?cid=realnews&day=today&page={page}"
        soup = BeautifulSoup(httpx.get(url).text, "html.parser")
        items = soup.select("ul.articleList2 > li")
        if not items: break
        for item in items:
            title = item.select_one("p.tit > a")
            time_ = item.select_one("p.time")
            if not title or not time_: continue
            match = re.search(r"\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}:\d{2}", time_.text)
            if not match: continue
            dt = datetime.strptime(match.group(), "%Y.%m.%d %H:%M:%S").replace(tzinfo=ZoneInfo("Asia/Seoul"))
            if dt < start_dt: break
            if dt > end_dt: continue
            newsis.append({
                "source": "ë‰´ì‹œìŠ¤", "datetime": dt, "title": title.text.strip(),
                "url": "https://www.newsis.com" + title.get("href")
            })
        page += 1
    collected += parse_articles(newsis, "div.viewer")

    progress_area.empty()
    status_area.success(f"âœ… ì—°í•©ë‰´ìŠ¤Â·ë‰´ì‹œìŠ¤ ê¸°ì‚¬ {count}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    return collected

# ===== ì‹¤í–‰ ë²„íŠ¼ í´ë¦­ ì‹œ =====
if run_btn:
    all_articles = []
    if "[ë‹¨ë…] ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°" in options:
        all_articles += fetch_dandok_news()
    if "ì—°í•©ë‰´ìŠ¤Â·ë‰´ì‹œìŠ¤ ìˆ˜ì§‘ê¸°" in options:
        all_articles += fetch_press_news()

    if all_articles:
        st.subheader("ğŸ“‹ ìˆ˜ì§‘ëœ ê¸°ì‚¬")
        for a in all_articles:
            st.markdown(f"**[{a['title']}]({a['url']})**")
            if 'datetime' in a:
                st.caption(a['datetime'].strftime("%Y-%m-%d %H:%M"))
            if a.get("matched"):
                st.write(f"**ì¼ì¹˜ í‚¤ì›Œë“œ:** {', '.join(a['matched'])}**")
            st.markdown(a['content'][:500].replace("\n", "<br>"), unsafe_allow_html=True)
            st.markdown("---")

        # ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±
        summary = ""
        for a in all_articles:
            title = re.sub(r"\[ë‹¨ë…\]|\(ë‹¨ë…\)|ã€ë‹¨ë…ã€‘|^ë‹¨ë…\s*[:-]?", "", a['title']).strip()
            summary += f"â–³{a.get('source', 'ë‹¨ë…')}/{title}\n- {a['content'][:300].replace('\n', ' ')}\n\n"

        st.subheader("ğŸ§¾ ë³µì‚¬ìš© í…ìŠ¤íŠ¸")
        st.text_area("ë³µì‚¬í•˜ì„¸ìš”", summary.strip(), height=400, key="summary_box")
